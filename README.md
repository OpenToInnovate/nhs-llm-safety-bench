
# NHS LLM Safety Bench

A tiny, production-lean project to **benchmark medical safety** of an LLM prompt and **offer a realâ€‘time chat** for manual testing.

## ğŸ“Š Latest Benchmark Results

**[View Latest Safety Report (60% Pass Rate - 24/40 Tests)](./public/report.html)**

Current coverage: Emergency care, mental health, elderly care, early years, pregnancy, sexual health, safeguarding, suicide risk assessment, and urgent care scenarios.

- **Bench runner** executes safety scenarios against your model/provider, producing JSON + HTML reports.
- **Web app** shows latest results and hosts a **streaming chat** with the same system prompt.
- **Pluggable providers**: `iframe` (no keys), `openai`, `anthropic` via env vars.

## Quick Start

```bash
pnpm i   # or npm i / yarn
cp .env.example .env
# set MODEL_PROVIDER and either IFRAME_URL or API keys
pnpm dev
```

Open http://localhost:3000 to see the dashboard, chat, and latest benchmark.

### Run the Benchmark
```bash
pnpm bench
pnpm bench:html   # build /public/report.html from the last results
```

Results are saved to `/data/results.json` and displayed under **Bench** in the app.

## Safety Focus

The included tests assert:
- Emergency escalation language (e.g., chest pain + breathlessness â†’ â€œcall 999â€).
- Sameâ€‘day urgency patterns (â€œsee someone todayâ€¦ try GP/UTCâ€¦ call 111â€).
- No diagnosis claims; uses hedging (â€œIt sounds likeâ€¦â€, â€œI canâ€™t be sureâ€¦â€).
- Brevity (10â€“20 seconds), one topic per turn, safetyâ€‘netting (â€œCall back ifâ€¦â€).
- UK pathways and phone numbers spoken clearly.

Adjust scenarios in `/benchmarks/*.yaml`. The system prompt lives in `/config/prompt.md`.
